<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>[数据分析]基于人物登场率生成《倚天》词云图 | Yhchdev</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">[数据分析]基于人物登场率生成《倚天》词云图</h1><a id="logo" href="/.">Yhchdev</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/jinengshu/"><i class="fa fa-tree"> 技能</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">[数据分析]基于人物登场率生成《倚天》词云图</h1><div class="post-meta">Aug 27, 2018</div><div class="post-content"><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>词云图在文本分析上有着显著的效果，前段时间看完了《倚天屠龙记》，这里使用Python通过<strong>jieba</strong>库进行小说全文分词,在通过<strong>wordcloud</strong>库基于小说中人物的登场效率(确切的说是人物名字的出现频率)生成词云图，登场率高的字体更大。先看看看效果，如下图：</p>
<p><img src="http://pbn3uskcn.bkt.clouddn.com/yitian.jpg" alt="词云图"></p>
<h4 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h4><h5 id="1-按照国际惯例Python库先装一波"><a href="#1-按照国际惯例Python库先装一波" class="headerlink" title="1.按照国际惯例Python库先装一波"></a>1.按照国际惯例Python库先装一波</h5><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line">pip3 <span class="keyword">install </span><span class="keyword">jieba</span></span><br><span class="line"><span class="keyword">pip3 </span><span class="keyword">install </span>worldcloud</span><br></pre></td></tr></table></figure>  
<p>woroldcloud默认是不支持中文的，需要下载一个中文字体,或者指定系统的中文字体路径</p>
<h5 id="2-小说txt文本"><a href="#2-小说txt文本" class="headerlink" title="2.小说txt文本"></a>2.小说txt文本</h5><p>点击即可下载</p>
<p><a href="https://wachat-1256238121.cos.ap-chengdu.myqcloud.com/yitian.zip" target="_blank" rel="noopener">倚天屠龙记.txt</a></p>
<h5 id="3-创建自定义关键词"><a href="#3-创建自定义关键词" class="headerlink" title="3.创建自定义关键词"></a>3.创建自定义关键词</h5><p>自定义字典文件 “namedict.txt”。在这个字典文件中，记录了几个小说人物名，并标注词性为”nr“，代表名字，举个例子，作用就是告诉程序周芷若是一个人的名字，jieba分词虽然功能强大，但是对于不常见的，特定的小说人物名称的断句和分词表现就不是那么亮眼了。格式如下：</p>
<p><img src="http://pbn3uskcn.bkt.clouddn.com/txt.png" alt="iamge"></p>
<h4 id="jieba和wordcloud功能简介"><a href="#jieba和wordcloud功能简介" class="headerlink" title="jieba和wordcloud功能简介"></a>jieba和wordcloud功能简介</h4><h5 id="1-中文分词组件jieba"><a href="#1-中文分词组件jieba" class="headerlink" title="1. 中文分词组件jieba"></a>1. 中文分词组件jieba</h5><p>功能1. 分词</p>
<ul>
<li><p>jieba.cut方法接受两个输入参数: 1) 第一个参数为需要分词的字符串 2）cut_all参数用来控制是否采用全模式</p>
</li>
<li><p>jieba.cut_for_search方法接受一个参数：需要分词的字符串,该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细</p>
</li>
<li><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line">txt = <span class="string">"我是西南林业大学一名大三的学生"</span></span><br><span class="line">seg_list = jieba.cut(txt,cut_all=True)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"[全模式]："</span>, <span class="string">"/"</span>.join(seg_list))</span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut(txt,cut_all= False)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"[精确模式]："</span>,<span class="string">"/"</span>.join(seg_list))</span><br><span class="line"></span><br><span class="line">seg_list = jieba.lcut_for_search(txt)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"[搜索引擎模式]:"</span>,<span class="string">'/'</span>.join(seg_list))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">[全模式]： 我<span class="regexp">/是/</span>西南<span class="regexp">/林业/</span>林业大学<span class="regexp">/业大/</span>大学<span class="regexp">/一名/</span>大三<span class="regexp">/的/</span>学生</span><br><span class="line">[精确模式]： 我<span class="regexp">/是/</span>西南<span class="regexp">/林业大学/</span>一名<span class="regexp">/大三/</span>的/学生</span><br><span class="line">[搜索引擎模式]: 我<span class="regexp">/是/</span>西南<span class="regexp">/林业/</span>业大<span class="regexp">/大学/</span>林业大学<span class="regexp">/一名/</span>大三<span class="regexp">/的/</span>学生</span><br><span class="line"></span><br></pre></td></tr></table></figure>  
</li>
</ul>
<p>可以看到就文本分析而言，精确模式分词效果是比较好的,当不指定cut_all参数时，默认为False,即默认为精确模式</p>
<p>功能2.添加自定义词典</p>
<ul>
<li><p>开发者可以指定自己自定义的词典，以便包含jieba词库里没有的词。虽然jieba有新词识别能力，但是自行添加新词可以保证更高的正确率</p>
</li>
<li><p>用法： jieba.load_userdict(file_name) # file_name为自定义词典的路径</p>
</li>
<li><p>词典格式和dict.txt一样，一个词占一行；每一行分三部分，一部分为词语，另一部分为词频，最后为词性（可省略），用空格隔开。</p>
</li>
</ul>
<p><strong>西南林业大学</strong>是一所大学的名称，不希望产生分词错误，通过创建dict.txt 里面输入“西南林业大学”</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">jieba.load_userdict(<span class="string">'dict.txt'</span>)</span><br><span class="line"></span><br><span class="line">txt = <span class="string">"我是西南林业大学一名大三的学生"</span></span><br><span class="line">seg_list = jieba.cut(txt)</span><br><span class="line"><span class="keyword">print</span>(<span class="string">"[通过自定义词典,避免分词错误]："</span>, <span class="string">"/"</span>.<span class="keyword">join</span>(seg_list))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">[通过自定义词典,避免分词错误]： 我<span class="regexp">/是/</span>西南林业大学<span class="regexp">/一名/</span>大三<span class="regexp">/的/</span>学生</span><br><span class="line"></span><br></pre></td></tr></table></figure>      
<p>功能3. 关键词提取</p>
<ul>
<li>jieba.analyse.extract_tags(sentence,topK) #需要先import jieba.analyse</li>
<li>setence为待提取的文本</li>
<li>topK为返回几个TF/IDF权重最大的关键词，默认值为20</li>
</ul>
<p>以小说分词和得到的文本信息，如下图，提取出现频率最大的三个名字</p>
<p><img src="http://pbn3uskcn.bkt.clouddn.com/02.jpg" alt="image"></p>
<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line"></span><br><span class="line">import jieba</span><br><span class="line">import jieba.analyse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">with open('分词后.txt') as f:</span><br><span class="line">    fc = f.read()</span><br><span class="line">topthree = jieba.analyse.extract_tags(fc,topK= 3)</span><br><span class="line"><span class="attribute">print('/'.join(topthree))</span></span><br><span class="line"><span class="attribute"></span></span><br><span class="line"><span class="attribute">&gt;&gt;&gt;</span></span><br><span class="line"><span class="attribute"></span></span><br><span class="line"><span class="attribute">张无忌/张翠山/谢逊</span></span><br><span class="line"><span class="attribute"></span></span><br></pre></td></tr></table></figure>  
<p>3.其他</p>
<p>分析词性功能，可以标注句子分词后每个词的词性，后续通过词性进行关键词提取;<br>并行分词<br>原理：将目标文本按行分隔后，把各行文本分配到多个python进程并行分词，然后归并结果，从而获得分词速度的可观提升,windows系统下还不支持</p>
<h4 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h4><p>Python join() 方法用于将序列中的元素以指定的字符连接生成一个新的字符串。</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line">seq = (<span class="string">'一'</span>,<span class="string">'二三'</span>,<span class="string">'四五六'</span>)</span><br><span class="line">print(<span class="string">'/'</span>.join(seq))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt;</span><br><span class="line">一/二三/四五六</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="symbol">seq:</span></span><br><span class="line">    print(i+<span class="string">'/'</span>,<span class="keyword">end</span> = <span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt;</span><br><span class="line">一/二三/四五六/</span><br><span class="line"></span><br></pre></td></tr></table></figure>       
<h5 id="2-词云生成库wordcloud"><a href="#2-词云生成库wordcloud" class="headerlink" title="2.词云生成库wordcloud"></a>2.词云生成库wordcloud</h5><p>这个库的使用就更简单了，只需要设置背景图片，字体，等参数，具体参数的设置可以参考文档，设置<strong>collocations=False</strong>参数是为了使词云图避免出现重复的关键词。</p>
<h5 id="3-matplotlib库"><a href="#3-matplotlib库" class="headerlink" title="3.matplotlib库"></a>3.matplotlib库</h5><p>python的一个画图库，这里用到只是为了显示生产的词云图，不用也行，可以将生产的词云图先保存下来，再在文件夹中找到打开进行显示</p>
<h4 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h4><p>词云中的词汇只有角色的名字，没有其他无关词汇，这样才能更好地反映出角色的权重。 </p>
<p>整个程序的实现思路：</p>
<pre><code>分词 (函数)
    小说文本整体分词（jieba）
    构建自定义人名词典
    筛选关键词(人名)
生成词云，绘制图片
</code></pre><h4 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">jieba.load_userdict(<span class="string">"namedict.txt"</span>)</span><br><span class="line"><span class="keyword">import</span> jieba.analyse <span class="keyword">as</span> analyse</span><br><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud</span><br><span class="line"><span class="keyword">from</span> scipy.misc <span class="keyword">import</span> imread</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取关键词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_words</span><span class="params">(file_name)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(file_name,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        fiction_text = f.read()</span><br><span class="line">    wordList = jieba.cut(fiction_text)  <span class="comment">#分词</span></span><br><span class="line">    print(<span class="string">'小说分词完成...'</span>)</span><br><span class="line">    allow_pos = (<span class="string">'nr'</span>,)                 <span class="comment">#设置筛选参数为”nr“</span></span><br><span class="line">    tags = jieba.analyse.extract_tags(fiction_text, topK=<span class="number">30</span>, withWeight=<span class="keyword">False</span>, allowPOS=allow_pos) <span class="comment">#从原文文本original_text中，筛选词性为”nr“的前30个词汇作为关键词</span></span><br><span class="line">    print(<span class="string">'关键词筛选完成...'</span>)</span><br><span class="line">    stags=<span class="string">"/"</span>.join(tags)                         <span class="comment">#将关键词用‘/’分隔</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"stags.txt"</span>,<span class="string">"w"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(stags)     <span class="comment">#将关键词保存到stags.txt文件中（可供调试查看）</span></span><br><span class="line">    outstr = <span class="string">''</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> wordList:</span><br><span class="line">        <span class="keyword">if</span> word  <span class="keyword">in</span> stags:   <span class="comment">#与关键词字符串比较，只保留关键词</span></span><br><span class="line">            <span class="keyword">if</span> word != <span class="string">'/'</span>:</span><br><span class="line">                outstr += word </span><br><span class="line">                outstr += <span class="string">"/"</span></span><br><span class="line">    <span class="keyword">return</span> outstr</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制词云</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_wordcloud</span><span class="params">(strwords)</span>:</span></span><br><span class="line">    backgroud_Image = plt.imread(<span class="string">'backpic.png'</span>)</span><br><span class="line">    cloud = WordCloud(width=<span class="number">1024</span>, height=<span class="number">768</span>,</span><br><span class="line">                 background_color=<span class="string">'white'</span>,mask=backgroud_Image,</span><br><span class="line">                 font_path=<span class="string">'/home/yhch/Downloads/kaiti.ttf'</span>,collocations=<span class="keyword">False</span>,</span><br><span class="line">                 max_font_size=<span class="number">400</span>,random_state=<span class="number">50</span>)</span><br><span class="line">    word_cloud = cloud.generate(strwords)                <span class="comment"># 生成词云数据</span></span><br><span class="line">    <span class="keyword">return</span> word_cloud</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    file_name = <span class="string">'/home/yhch/Downloads/yitian.txt'</span></span><br><span class="line">    outstr = get_words(file_name)</span><br><span class="line">    word_cloud=draw_wordcloud(outstr)</span><br><span class="line">    plt.imshow(word_cloud)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    word_cloud.to_file(<span class="string">'yitian.jpg'</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>  
<h4 id="分析结果"><a href="#分析结果" class="headerlink" title="分析结果"></a>分析结果</h4><p>生成的词云</p>
<p><img src="http://pbn3uskcn.bkt.clouddn.com/yitian.jpg" alt="词云图"></p>
<p>因为分析的结果是图片，直观，简单，一目了然，这里就不再赘述了，同时也暴露出了小问题这里把<strong>武功</strong> ,  <strong>少林</strong> 等关键词也当成了人名进行操作，wordcloud有停用词方法，可以对不想显示的关键词语进行屏蔽;同时程序也有需要改进的地方，整个程序大概要运行2分钟才能结束，一部分原因是这部小说有一百万字，数据量还是比较大的，程序中频繁的IO操作也是有影响速度的主要因素，是考虑到将中间结果保存下来，可供调试，就懒得。先这样后续会改进更新。</p>
</div><iframe src="/donate/?AliPayQR=/img/AliPayQR.jpg&amp;WeChatQR=/img/WeChatQR.png&amp;GitHub=null&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden; overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><div class="tags"></div><div class="post-nav"><a class="pre" href="/2018/09/02/数据可视化-Seaborn简单介绍/">[数据可视化]Seaborn简单介绍</a><a class="next" href="/2018/08/19/Ubuntu-配置仿MacOS主题/">[Ubuntu]配置仿MacOS主题</a></div><div id="lv-container" data-id="city" data-uid="MTAyMC8zNTcwNS8xMjI0MQ=="><script>(function(d, s) {
   var j, e = d.getElementsByTagName(s)[0];
   if (typeof LivereTower === 'function') { return; }
   j = d.createElement(s);
   j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
   j.async = true;
   e.parentNode.insertBefore(j, e);
})(document, 'script');
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="http://yoursite.com"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/09/04/验证码识别-图形验证码识别01/">[验证码识别]图形验证码识别01</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/02/数据可视化-Seaborn简单介绍/">[数据可视化]Seaborn简单介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/27/数据分析-基于人物登场率生成《倚天》词云图/">[数据分析]基于人物登场率生成《倚天》词云图</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/19/Ubuntu-配置仿MacOS主题/">[Ubuntu]配置仿MacOS主题</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/07/Python3爬虫-selenium爬取淘宝商品信息/">[Python3爬虫]selenium爬取淘宝商品信息</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/03/JSP-搭建开发环境/">[JSP]搭建开发环境</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/28/随笔-倚天屠龙记读后感/">[随笔]倚天屠龙记读后感</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/19/Design-Pattern-单例模式/">[Design Pattern]单例模式</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/17/Design-Pattern-抽象工厂模式/">[Design Pattern]抽象工厂模式</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/10/DesignPattern-FactoryPattern/">[DesignPattern]FactoryPattern</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">Yhchdev.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async="async"></script><span id="busuanzi_container_site_pv">|访问量<span id="busuanzi_value_site_pv"></span></span><span id="busuanzi_container_site_uv">|访客数<span id="busuanzi_value_site_uv"> </span></span></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.2.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.2.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>